# AI API Load Testing

This repository contains tools for load testing AI APIs using Locust. Currently, it supports testing OpenAI and Hugging Face APIs.

## Setup

1. Clone this repository
2. Install dependencies:
```bash
pip install locust requests python-dotenv pandas matplotlib
```
3. Configure your API keys in the `.env` file:
```bash
# Copy the example .env file
cp .env.example .env

# Edit the .env file with your API keys
nano .env
```

## API Keys Configuration

The project uses a `.env` file in the root directory to store API keys and other configuration. This is more secure than hardcoding keys in the source files.

Example `.env` file:
```
# API Keys for Load Testing
# Replace with your actual API keys

# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Hugging Face API Key
HF_API_KEY=your_huggingface_api_key_here

# Other configuration
OPENAI_MODEL=gpt-3.5-turbo
HF_MODEL=gpt2
```

## Directory Structure

- `locustfiles/` - Contains Locust test files for different APIs
  - `openai_locustfile.py` - Load test for OpenAI API
  - `huggingface_locustfile.py` - Load test for Hugging Face
  - `verify_api_keys.py` - Script to verify API keys before running tests
- `.env` - Environment variables for API keys and configuration
- `run_all_tests.sh` - Script to run all tests and generate reports
- `analyze_reports.py` - Script to analyze test results and generate comparison reports

## Verifying API Keys

Before running load tests, you can verify that your API keys are valid:

```bash
cd locustfiles
python verify_api_keys.py
```

This script will:
- Test your OpenAI API key with a simple request
- Test your Hugging Face API key (if using the inference API)
- Provide detailed error messages if any keys are invalid

## Running Load Tests

### OpenAI API

To test the OpenAI API:

```bash
cd locustfiles
python openai_locustfile.py
```

This will start the Locust web interface at http://localhost:8089. From there, you can:
1. Set the number of users to simulate
2. Set the spawn rate (users per second)
3. Start the test and monitor results in real-time

### Hugging Face

To test the Hugging Face API:

```bash
cd locustfiles
python huggingface_locustfile.py
```

This will start the Locust web interface at http://localhost:8089 (or another port if 8089 is already in use).

### Running All Tests

To run all tests and generate reports:

```bash
./run_all_tests.sh
```

You can customize the test duration, number of users, and spawn rate:

```bash
./run_all_tests.sh 5m 20 5
# Runs tests for 5 minutes with 20 users at a spawn rate of 5 users/second
```

## Analyzing Test Results

After running the tests, you can analyze the results using the `analyze_reports.py` script:

```bash
python analyze_reports.py
```

This script will:
1. Read the CSV reports generated by Locust
2. Generate a performance comparison between OpenAI and Hugging Face
3. Provide insights and recommendations based on the test results
4. Create a visualization of response times and throughput over time

The analysis includes:
- Comparison of response times (average, median, min, max)
- Throughput comparison (requests per second)
- Stability analysis (consistency of response times)
- Failure rate analysis
- Recommendations for production use

The script generates a performance comparison plot in the `reports/` directory.

## Running Tests from Command Line

You can also run tests without the web interface:

```bash
cd locustfiles
locust -f openai_locustfile.py --headless -u 10 -r 1 -t 30s
```

Where:
- `-u 10` - Simulates 10 users
- `-r 1` - Spawns 1 user per second
- `-t 30s` - Runs the test for 30 seconds

## Customizing Tests

### OpenAI API

Edit the `.env` file to:
- Change the API key
- Modify the model being used

Edit `locustfiles/openai_locustfile.py` to:
- Adjust the prompt
- Change the max tokens
- Add additional parameters

### Hugging Face

Edit the `.env` file to:
- Change the API key (if using inference API)
- Change the model being tested

Edit `locustfiles/huggingface_locustfile.py` to:
- Modify the input text
- Adjust parameters

## Generating Reports

To generate HTML reports:

```bash
cd locustfiles
locust -f openai_locustfile.py --headless -u 10 -r 1 -t 1m --html=../reports/openai_report.html
```

This will create a report in the `reports/` directory.

## Common Issues and Solutions

### 403 Forbidden Errors
- Verify your API key is valid using the verification script
- Check if your API key has the necessary permissions
- Ensure you're using the correct API endpoint

### Rate Limiting
- Reduce the number of concurrent users
- Add delays between requests
- Check your API usage limits

### Connection Errors
- Verify your internet connection
- Check if the API service is experiencing downtime
- Ensure you're using the correct host URL

## Security Best Practices

1. **Never commit your `.env` file to version control**
   - Add `.env` to your `.gitignore` file
   - Only commit `.env.example` with placeholder values

2. **Rotate API keys regularly**
   - Generate new API keys periodically
   - Revoke old keys after rotation

3. **Use environment-specific keys**
   - Use different API keys for development and production
   - Consider using key restrictions (IP, usage limits, etc.)

## Advanced Configuration

### Environment Variables

You can use environment variables for sensitive information:

```bash
export OPENAI_API_KEY="your-api-key"
export HF_API_KEY="your-huggingface-key"
```

Then modify the locustfiles to use:
```python
import os
api_key = os.environ.get("OPENAI_API_KEY")
```

### Distributed Load Testing

For larger tests, you can run Locust in distributed mode:

1. Start a master:
```bash
locust -f openai_locustfile.py --master
```

2. Start workers:
```bash
locust -f openai_locustfile.py --worker --master-host=localhost
``` 